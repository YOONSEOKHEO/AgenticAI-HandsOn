{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f410f5-f1c5-4bce-be4c-1bd4b9a2e924",
   "metadata": {},
   "source": [
    "# Session 3: 예측 모델 개발 및 학습 (Model Development & Training)\n",
    "\n",
    "### ■ Session Objective\n",
    "- Vertex AI 개발 환경에서 금융 데이터를 기반으로 대출 승인 여부를 예측하는 머신러닝 모델을 개발하고, 학습된 모델 아티팩트(Artifact)를 생성함.\n",
    "\n",
    "### ■ Key Process\n",
    "1.  **Data Ingestion (데이터 로드)**\n",
    "    - Cloud Storage(GCS)에 적재된 원본 데이터셋(`loan_data.csv`)을 Pandas DataFrame으로 로드.\n",
    "\n",
    "2.  **Data Preprocessing (데이터 전처리)**\n",
    "    - 결측치 처리, 범주형 변수 인코딩 등 모델 학습에 적합한 형태로 데이터를 정제 및 변환.\n",
    "    - 모델의 입력 변수(Features)와 목표 변수(Target)를 정의하고 분리.\n",
    "\n",
    "3.  **Model Training (모델 학습)**\n",
    "    - Scikit-learn 라이브러리의 `RandomForestClassifier` 알고리즘을 사용하여 분류 모델을 학습.\n",
    "    - 학습용(Train) 데이터와 검증용(Test) 데이터를 분리하여 모델 성능을 객관적으로 평가.\n",
    "\n",
    "4.  **Model Serialization (모델 직렬화)**\n",
    "    - 학습 완료된 모델 객체(Trained Model)를 추후 배포 및 재사용이 가능한 파일(`.pkl`) 형태로 직렬화하여 저장.\n",
    "\n",
    "### ■ Deliverable\n",
    "- `model.pkl`: 배포 및 서빙을 위해 준비된 직렬화된 모델 아티팩트.\n",
    "- `model_features.pkl`: 추론 시점의 데이터 일관성을 보장하기 위한 피처 목록 파일.# Session 3: 예측 모델 개발 및 학습 (Model Development & Training)\n",
    "\n",
    "### ■ Session Objective\n",
    "- Vertex AI 개발 환경에서 금융 데이터를 기반으로 대출 승인 여부를 예측하는 머신러닝 모델을 개발하고, 학습된 모델 아티팩트(Artifact)를 생성함.\n",
    "\n",
    "### ■ Key Process\n",
    "1.  **Data Ingestion (데이터 로드)**\n",
    "    - Cloud Storage(GCS)에 적재된 원본 데이터셋(`train_loan_prediction.csv`)을 Pandas DataFrame으로 로드.\n",
    "\n",
    "2.  **Data Preprocessing (데이터 전처리)**\n",
    "    - 결측치 처리, 범주형 변수 인코딩 등 모델 학습에 적합한 형태로 데이터를 정제 및 변환.\n",
    "    - 모델의 입력 변수(Features)와 목표 변수(Target)를 정의하고 분리.\n",
    "\n",
    "3.  **Model Training (모델 학습)**\n",
    "    - Scikit-learn 라이브러리의 `RandomForestClassifier` 알고리즘을 사용하여 분류 모델을 학습.\n",
    "    - 학습용(Train) 데이터와 검증용(Test) 데이터를 분리하여 모델 성능을 객관적으로 평가.\n",
    "\n",
    "4.  **Model Serialization (모델 직렬화)**\n",
    "    - 학습 완료된 모델 객체(Trained Model)를 추후 배포 및 재사용이 가능한 파일(`.pkl`) 형태로 직렬화하여 저장.\n",
    "\n",
    "### ■ Deliverable\n",
    "- `model.pkl`: 배포 및 서빙을 위해 준비된 직렬화된 모델 아티팩트.\n",
    "- `model_features.pkl`: 추론 시점의 데이터 일관성을 보장하기 위한 피처 목록 파일."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba70f4-92d3-4f67-8a02-50ce93d9958d",
   "metadata": {},
   "source": [
    "## 0. 사전 준비 (Prerequisites)\n",
    "\n",
    "원활한 실습 진행을 위해, 교육 시작 전 아래 두 가지 준비 사항을 반드시 완료해주시기 바랍니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 실습 데이터셋 다운로드\n",
    "\n",
    "-   **데이터셋 출처:** [Loan Prediction Problem Dataset](https://www.kaggle.com/datasets/altruistdelhite04/loan-prediction-problem-dataset)\n",
    "-   **Github:** [세션3-데이터셋](https://github.com/YOONSEOKHEO/AgenticAI-HandsOn/blob/main/1_VertexAI/Session3/loan_data/train_loan_prediction.csv)\n",
    "-   **필수 파일:** `train_loan_prediction.csv`\n",
    "    -   *상기 링크에 접속하여 `train_loan_prediction.csv` 파일을 로컬 PC에 다운로드합니다.*  \n",
    "    -   오른쪽 상단 다운로드 버튼(Download_raw_file)\n",
    "    -   만약 git clone을 로컬에 해두었다면 다운로드 할 필요 없음\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Google Cloud Storage(GCS) 버킷 생성\n",
    "\n",
    "-   **목적:** 다운로드한 데이터셋을 업로드하고, 향후 생성될 모델 아티팩트를 저장하기 위한 클라우드 스토리지 공간을 확보합니다.\n",
    "\n",
    "-   **생성 절차:**\n",
    "    1.  GCP Console 접속 후, 상단 검색창을 통해 'Cloud Storage' 서비스로 이동합니다.\n",
    "    2. 왼쪽 바에 **2.버킷\n",
    "    2.  `+ 만들기` 버튼을 클릭하여 버킷 생성 프로세스를 시작합니다.\n",
    "    3.  아래 가이드에 따라 버킷 속성을 설정합니다.\n",
    "        -   **이름 (Name):** 전역적으로 고유한(Globally unique) 이름으로 설정\n",
    "            -   *권장 형식: `[GCP-Project-ID]-[이름이니셜]-train-loan-data`*\n",
    "            -   *예시: `my_instance-ysh-train-loan-data`*\n",
    "        -   **위치 유형 (Location type):** `Region`\n",
    "        -   **리전 (Region):** `asia-northeast3 (Seoul)`\n",
    "        -   *나머지 설정은 모두 기본값을 유지합니다.*\n",
    "    4.  `만들기` 버튼을 클릭하여 버킷 생성을 완료합니다.\n",
    "    5.  `업로드 버튼을 통해 대출 심사 데이터 업로드 하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa82cb90-0f78-40a1-a4d6-8946b07809a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa1fd68-f8bb-40fd-b7f9-8c817b431d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: 필요한 라이브러리를 모두 가져왔습니다.\n",
      "현재 scikit-learn 버전: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Session 3: AI 대출 심사관 모델 학습 (Full Code)\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 단계 1: 라이브러리 가져오기 및 환경 설정\n",
    "# ==============================================================================\n",
    "# 데이터 분석을 위한 pandas, 모델 저장을 위한 joblib 등을 가져옵니다.\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib, pickle\n",
    "import os\n",
    "import sys # 시스템 관련 함수를 사용하기 위해 임포트\n",
    "\n",
    "print(\"Step 1: 필요한 라이브러리를 모두 가져왔습니다.\")\n",
    "\n",
    "# --- scikit-learn 버전 검사 및 조정 ---\n",
    "REQUIRED_SKLEARN_VERSION = \"1.5.2\" # Vertex AI Pre-built Container와 호환되는 버전\n",
    "\n",
    "current_sklearn_version = sklearn.__version__\n",
    "print(f\"현재 scikit-learn 버전: {current_sklearn_version}\")\n",
    "\n",
    "if current_sklearn_version != REQUIRED_SKLEARN_VERSION:\n",
    "    print(f\"경고: 필요한 scikit-learn 버전({REQUIRED_SKLEARN_VERSION})과 다릅니다.\")\n",
    "    print(f\"     자동으로 '{REQUIRED_SKLEARN_VERSION}' 버전으로 다운그레이드/설치를 진행합니다.\")\n",
    "    \n",
    "    # pip를 사용하여 지정된 버전 설치\n",
    "    !pip install scikit-learn=={REQUIRED_SKLEARN_VERSION} --quiet\n",
    "    \n",
    "    # 설치 후, Jupyter 커널을 다시 시작하도록 안내 (변경 사항 적용을 위함)\n",
    "    print(f\"\\n====================================================================================\")\n",
    "    print(f\"▶ scikit-learn {REQUIRED_SKLEARN_VERSION} 설치 완료! \")\n",
    "    print(f\"▶ 변경 사항 적용을 위해 JupyterLab 'Kernel' -> 'Restart Kernel...'을 클릭하여\")\n",
    "    print(f\"▶ 커널을 반드시 '다시 시작' 해주세요. 그 후에 다시 이 셀부터 실행하시면 됩니다.\")\n",
    "    print(f\"====================================================================================\")\n",
    "    \n",
    "    # 커널 재시작 메시지를 출력하고 스크립트 실행을 중단하여 사용자가 재시작하도록 유도\n",
    "    # (실제 환경에서는 sys.exit()를 사용할 수 있지만, Jupyter에서는 권장하지 않습니다.)\n",
    "    # 여기서는 안내 메시지를 출력하는 것으로 충분합니다.\n",
    "    # sys.exit() # JupyterLab에서 sys.exit()는 셀 실행만 중단하고 커널을 죽이지 않습니다.\n",
    "\n",
    "# --- (선택 사항: 재시작 후 버전 재확인) ---\n",
    "# 커널 재시작 후 이 셀을 다시 실행하면, 아래 라인에서\n",
    "# \"scikit-learn 버전이 일치합니다.\" 메시지를 볼 수 있습니다.\n",
    "# import sklearn # 재시작 후에는 다시 임포트해야 최신 버전 반영\n",
    "# if sklearn.__version__ == REQUIRED_SKLEARN_VERSION:\n",
    "#     print(f\"scikit-learn 버전이 {REQUIRED_SKLEARN_VERSION}으로 일치합니다.\")\n",
    "# else:\n",
    "#     print(f\"오류: scikit-learn 버전이 여전히 일치하지 않습니다. 수동 재시작을 확인해주세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222cd785-c79e-4cbe-831f-034ce7400ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: GCS에서 데이터를 성공적으로 불러왔습니다.\n",
      "  - 원본 데이터 (Train) : 614 건\n",
      "\n",
      "  - 원본 데이터 미리보기:\n",
      "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001002   Male      No          0      Graduate            No   \n",
      "1  LP001003   Male     Yes          1      Graduate            No   \n",
      "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
      "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
      "4  LP001008   Male      No          0      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0         NaN             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area Loan_Status  \n",
      "0             1.0         Urban           Y  \n",
      "1             1.0         Rural           N  \n",
      "2             1.0         Urban           Y  \n",
      "3             1.0         Urban           Y  \n",
      "4             1.0         Urban           Y  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 단계 2: GCS에서 데이터 불러오기\n",
    "# ------------------------------------------------------------------------------\n",
    "# Google Cloud Storage 버킷에 있는 데이터를 직접 읽어옵니다.\n",
    "# 'gs://' 프로토콜을 사용하면 됩니다.\n",
    "\n",
    "# ※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n",
    "# ※ 수강생 각자의 버킷 이름으로 이 부분을 수정해야 합니다! ※\n",
    "# ※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n",
    "\n",
    "BUCKET_NAME = \"my_instance-ysh2-banking-data\"  # 예: \"gcp-project-12345-loan-data\"\n",
    "TRAIN_FILE = \"train_loan_prediction.csv\"\n",
    "GCS_PATH = f\"gs://{BUCKET_NAME}/{TRAIN_FILE}\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(GCS_PATH)\n",
    "    print(\"\\nStep 2: GCS에서 데이터를 성공적으로 불러왔습니다.\")\n",
    "    print(f\"  - 원본 데이터 (Train) : {df.shape[0]} 건\")\n",
    "    print(\"\\n  - 원본 데이터 미리보기:\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"\\nStep 2: 데이터 로드 중 오류 발생! 버킷 이름과 파일 이름을 확인하세요: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775b3d53-564b-469f-b784-3505f76b6f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: 데이터 전처리를 시작합니다.\n",
      "  - 데이터 전처리가 완료되었습니다.\n",
      "\n",
      "  - 전처리된 피처(X) 데이터 미리보기 (앞 5개 컬럼):\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5849                0.0       128.0             360.0   \n",
      "1             4583             1508.0       128.0             360.0   \n",
      "2             3000                0.0        66.0             360.0   \n",
      "3             2583             2358.0       120.0             360.0   \n",
      "4             6000                0.0       141.0             360.0   \n",
      "\n",
      "   Credit_History  \n",
      "0             1.0  \n",
      "1             1.0  \n",
      "2             1.0  \n",
      "3             1.0  \n",
      "4             1.0  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 단계 3: 데이터 전처리 (Data Preprocessing) - (오류 수정 코드)\n",
    "# ------------------------------------------------------------------------------\n",
    "# AI가 학습할 수 있도록 데이터를 깨끗하게 다듬는 과정입니다.\n",
    "\n",
    "print(\"\\nStep 3: 데이터 전처리를 시작합니다.\")\n",
    "\n",
    "# 1. 불필요한 'Loan_ID' 컬럼 제거\n",
    "#    errors='ignore'를 추가하여, 컬럼이 이미 삭제된 경우에도 오류 없이 넘어갑니다.\n",
    "df = df.drop('Loan_ID', axis=1, errors='ignore') # <-- 여기가 수정되었습니다.\n",
    "\n",
    "# 2. 결측치(NaN) 처리\n",
    "#    - 범주형 컬럼은 최빈값(mode)으로 채움\n",
    "cat_cols = ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "#    - 수치형 컬럼은 중앙값(median)으로 채움\n",
    "num_cols = ['LoanAmount', 'Loan_Amount_Term']\n",
    "for col in num_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# 3. 범주형 변수 인코딩 (One-Hot Encoding)\n",
    "#    (타겟 변수인 Loan_Status는 제외하고 피처들만 인코딩)\n",
    "df_processed = pd.get_dummies(df, drop_first=True, columns=[\n",
    "    'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'\n",
    "])\n",
    "\n",
    "# 4. 타겟 변수(Loan_Status)를 1(Y)과 0(N)으로 변환\n",
    "df_processed['Loan_Status'] = df_processed['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# 5. 피처(X)와 타겟(y) 분리\n",
    "X = df_processed.drop('Loan_Status', axis=1)\n",
    "y = df_processed['Loan_Status']\n",
    "\n",
    "print(\"  - 데이터 전처리가 완료되었습니다.\")\n",
    "print(\"\\n  - 전처리된 피처(X) 데이터 미리보기 (앞 5개 컬럼):\")\n",
    "print(X.head(5).iloc[:, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7699e051-bc4f-49cd-abec-3b6192eeefe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Loan_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6986cc7-0bd5-4f97-90bc-192646e63247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: 데이터를 학습용과 검증용으로 분리했습니다.\n",
      "  - 학습용 데이터 (X_train): 491 건\n",
      "  - 검증용 데이터 (X_test) : 123 건\n",
      "\n",
      "Step 5: AI 모델 학습을 시작합니다.\n",
      "  - AI 모델 학습을 완료했습니다.\n",
      "\n",
      "Step 6: 검증용 데이터로 모델 성능을 평가합니다.\n",
      "  - Accuracy (정확도): 0.8374\n",
      "\n",
      "  - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Refused (N)       0.80      0.63      0.71        38\n",
      "Approved (Y)       0.85      0.93      0.89        85\n",
      "\n",
      "    accuracy                           0.84       123\n",
      "   macro avg       0.82      0.78      0.80       123\n",
      "weighted avg       0.83      0.84      0.83       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 단계 4: 모델 학습 (Model Training)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# 전체 데이터를 학습용(80%)과 검증용(20%)으로 분리합니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"\\nStep 4: 데이터를 학습용과 검증용으로 분리했습니다.\")\n",
    "print(f\"  - 학습용 데이터 (X_train): {X_train.shape[0]} 건\")\n",
    "print(f\"  - 검증용 데이터 (X_test) : {X_test.shape[0]} 건\")\n",
    "\n",
    "\n",
    "# '학습용' 데이터(X_train, y_train)를 사용하여 모델을 학습시킵니다.\n",
    "print(\"\\nStep 5: AI 모델 학습을 시작합니다.\")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"  - AI 모델 학습을 완료했습니다.\")\n",
    "\n",
    "# 학습에 사용되지 않은 '검증용' 데이터(X_test, y_test)로 모델의 성능을 공정하게 평가합니다.\n",
    "print(\"\\nStep 6: 검증용 데이터로 모델 성능을 평가합니다.\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"  - Accuracy (정확도): {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n  - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Refused (N)', 'Approved (Y)']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e5626c-5ffe-437b-9619-16326bc68e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7: 새로운 가상 데이터로 예측을 시뮬레이션합니다.\n",
      "\n",
      "  [1. Raw Sample Data]\n",
      "  Gender Married Dependents Education Self_Employed  ApplicantIncome  \\\n",
      "0   Male     Yes          1  Graduate            No             5000   \n",
      "\n",
      "   CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n",
      "0               2000         150             360.0             1.0   \n",
      "\n",
      "  Property_Area  \n",
      "0         Urban  \n",
      "\n",
      "  [2. Preprocessed Sample Data for Model (앞 5개 컬럼)]\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             5000               2000         150             360.0   \n",
      "\n",
      "   Credit_History  \n",
      "0             1.0  \n",
      "\n",
      "  [3. Prediction Result]\n",
      "  -> 예측 결과: '승인 (Approved - Y)'\n",
      "  -> '거절(N)' 확률: 19.00%\n",
      "  -> '승인(Y)' 확률: 81.00%\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 단계 7: 새로운 가상 데이터로 예측 시뮬레이션 (Prediction Simulation)\n",
    "# ------------------------------------------------------------------------------\n",
    "# 모델이 새로운 데이터를 어떻게 예측하는지 시뮬레이션합니다.\n",
    "print(\"\\nStep 7: 새로운 가상 데이터로 예측을 시뮬레이션합니다.\")\n",
    "\n",
    "# 1. 가상의 대출 신청자 데이터 정의 (Raw Data)\n",
    "sample_data = {\n",
    "    'Gender': 'Male', 'Married': 'Yes', 'Dependents': '1', 'Education': 'Graduate',\n",
    "    'Self_Employed': 'No', 'ApplicantIncome': 5000, 'CoapplicantIncome': 2000,\n",
    "    'LoanAmount': 150, 'Loan_Amount_Term': 360.0, 'Credit_History': 1.0,\n",
    "    'Property_Area': 'Urban'\n",
    "}\n",
    "sample_df = pd.DataFrame([sample_data])\n",
    "print(\"\\n  [1. Raw Sample Data]\")\n",
    "print(sample_df)\n",
    "\n",
    "# 2. Raw Data -> 전처리 적용 (One-Hot Encoding)\n",
    "sample_df_processed = pd.get_dummies(sample_df)\n",
    "\n",
    "# 3. 피처 정렬 (Feature Alignment) - ⭐ 매우 중요 ⭐\n",
    "#    학습 시 사용된 'X.columns'를 기준으로 컬럼을 재정렬하고, 없는 컬럼은 0으로 채웁니다.\n",
    "final_sample = sample_df_processed.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "print(\"\\n  [2. Preprocessed Sample Data for Model (앞 5개 컬럼)]\")\n",
    "print(final_sample[final_sample.columns[:5]])\n",
    "\n",
    "# 4. 최종 예측 수행\n",
    "prediction = model.predict(final_sample)\n",
    "prediction_proba = model.predict_proba(final_sample)\n",
    "\n",
    "print(\"\\n  [3. Prediction Result]\")\n",
    "if prediction[0] == 1:\n",
    "    print(f\"  -> 예측 결과: '승인 (Approved - Y)'\")\n",
    "else:\n",
    "    print(f\"  -> 예측 결과: '거절 (Refused - N)'\")\n",
    "\n",
    "print(f\"  -> '거절(N)' 확률: {prediction_proba[0][0]:.2%}\")\n",
    "print(f\"  -> '승인(Y)' 확률: {prediction_proba[0][1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c87b95-ee0f-4375-b209-86063f520e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 8: 학습된 모델을 파일로 저장합니다.\n",
      "  - 모델이 'models_debug2/model/model.pkl' 경로에 성공적으로 저장되었습니다.\n",
      "  - JupyterLab 왼쪽 파일 탐색기에서 'models' 폴더와 그 안의 파일들을 확인하세요!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 단계 8: 학습된 모델 저장하기 (Model Saving)\n",
    "# ------------------------------------------------------------------------------\n",
    "# 'models' 폴더를 만들어 학습된 모델과 피처 목록을 저장합니다.\n",
    "print(\"\\nStep 8: 학습된 모델을 파일로 저장합니다.\")\n",
    "\n",
    "MODEL_DIR = \"models_debug2\"\n",
    "MODEL_DIR_ONLY = MODEL_DIR + '/model'\n",
    "MODEL_FEATURE_DIR = MODEL_DIR+'/feat'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR_ONLY, exist_ok=True)\n",
    "os.makedirs(MODEL_FEATURE_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_FILE_PATH = os.path.join(MODEL_DIR_ONLY, 'model.pkl')  # pkl 파일명 절대 수정하지 말것 \n",
    "FEATURES_FILE_PATH = os.path.join(MODEL_FEATURE_DIR, 'model_features.pkl') # pkl 파일명 절대 수정하지 말것 \n",
    "\n",
    "# 순수 pickle로 저장\n",
    "with open(MODEL_FILE_PATH, \"wb\") as f:\n",
    "    pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# 피처 목록도 동일하게 pickle로 저장\n",
    "with open(FEATURES_FILE_PATH, \"wb\") as f:\n",
    "    pickle.dump(list(X.columns), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "#joblib.dump(model, MODEL_FILE_PATH)\n",
    "#joblib.dump(list(X.columns), FEATURES_FILE_PATH) # 예측 시 사용할 피처 순서 저장\n",
    "\n",
    "print(f\"  - 모델이 '{MODEL_FILE_PATH}' 경로에 성공적으로 저장되었습니다.\")\n",
    "print(\"  - JupyterLab 왼쪽 파일 탐색기에서 'models' 폴더와 그 안의 파일들을 확인하세요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae7937-1080-4883-8e83-29f56a61cc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba823092-5b4c-49fb-bf0d-fc609c7fa25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
